{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e7e2c49",
   "metadata": {},
   "source": [
    "# Time Series Analisis Audio Multilable\n",
    "\n",
    "## Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a057144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\fikri\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.1.4)\n",
      "Requirement already satisfied: librosa in c:\\users\\fikri\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.11.0)\n",
      "Requirement already satisfied: tsfel in c:\\users\\fikri\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.2.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\fikri\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (4.67.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\fikri\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.4.2)\n",
      "Requirement already satisfied: joblib in c:\\users\\fikri\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.3.2)\n",
      "Requirement already satisfied: numpy<2,>=1.23.2 in c:\\users\\fikri\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\fikri\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\fikri\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\fikri\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: audioread>=2.1.9 in c:\\users\\fikri\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from librosa) (3.1.0)\n",
      "Requirement already satisfied: numba>=0.51.0 in c:\\users\\fikri\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from librosa) (0.61.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\fikri\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from librosa) (1.11.4)\n",
      "Requirement already satisfied: decorator>=4.3.0 in c:\\users\\fikri\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from librosa) (5.2.1)\n",
      "Requirement already satisfied: soundfile>=0.12.1 in c:\\users\\fikri\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from librosa) (0.13.1)\n",
      "Requirement already satisfied: pooch>=1.1 in c:\\users\\fikri\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from librosa) (1.8.2)\n",
      "Requirement already satisfied: soxr>=0.3.2 in c:\\users\\fikri\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from librosa) (1.0.0)\n",
      "Requirement already satisfied: typing_extensions>=4.1.1 in c:\\users\\fikri\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from librosa) (4.12.2)\n",
      "Requirement already satisfied: lazy_loader>=0.1 in c:\\users\\fikri\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from librosa) (0.4)\n",
      "Requirement already satisfied: msgpack>=1.0 in c:\\users\\fikri\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from librosa) (1.1.2)\n",
      "Requirement already satisfied: ipython>=7.4.0 in c:\\users\\fikri\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tsfel) (9.4.0)\n",
      "Requirement already satisfied: PyWavelets>=1.4.1 in c:\\users\\fikri\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tsfel) (1.9.0)\n",
      "Requirement already satisfied: requests>=2.31.0 in c:\\users\\fikri\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tsfel) (2.32.3)\n",
      "Requirement already satisfied: setuptools>=47.1.1 in c:\\users\\fikri\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tsfel) (65.5.0)\n",
      "Requirement already satisfied: statsmodels>=0.12.0 in c:\\users\\fikri\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tsfel) (0.14.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\fikri\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\fikri\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: ipython-pygments-lexers in c:\\users\\fikri\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from ipython>=7.4.0->tsfel) (1.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\fikri\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from ipython>=7.4.0->tsfel) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\users\\fikri\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from ipython>=7.4.0->tsfel) (0.1.7)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in c:\\users\\fikri\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from ipython>=7.4.0->tsfel) (3.0.51)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\fikri\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from ipython>=7.4.0->tsfel) (2.19.2)\n",
      "Requirement already satisfied: stack_data in c:\\users\\fikri\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from ipython>=7.4.0->tsfel) (0.6.3)\n",
      "Requirement already satisfied: traitlets>=5.13.0 in c:\\users\\fikri\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from ipython>=7.4.0->tsfel) (5.14.3)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\fikri\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=7.4.0->tsfel) (0.2.13)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in c:\\users\\fikri\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jedi>=0.16->ipython>=7.4.0->tsfel) (0.8.5)\n",
      "Requirement already satisfied: packaging in c:\\users\\fikri\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from lazy_loader>=0.1->librosa) (24.2)\n",
      "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in c:\\users\\fikri\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from numba>=0.51.0->librosa) (0.44.0)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in c:\\users\\fikri\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pooch>=1.1->librosa) (4.4.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\fikri\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\fikri\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.31.0->tsfel) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\fikri\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.31.0->tsfel) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\fikri\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.31.0->tsfel) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\fikri\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.31.0->tsfel) (2025.1.31)\n",
      "Requirement already satisfied: cffi>=1.0 in c:\\users\\fikri\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from soundfile>=0.12.1->librosa) (1.17.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\fikri\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.22)\n",
      "Requirement already satisfied: patsy>=0.5.6 in c:\\users\\fikri\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from statsmodels>=0.12.0->tsfel) (1.0.1)\n",
      "Requirement already satisfied: executing>=1.2.0 in c:\\users\\fikri\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from stack_data->ipython>=7.4.0->tsfel) (2.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in c:\\users\\fikri\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from stack_data->ipython>=7.4.0->tsfel) (3.0.0)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\fikri\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from stack_data->ipython>=7.4.0->tsfel) (0.2.3)\n",
      "Cell 1: Semua library berhasil di-import.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!pip install pandas librosa tsfel tqdm scikit-learn joblib\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import tsfel\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.model_selection import KFold, cross_validate, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.metrics import make_scorer, recall_score, f1_score, mean_absolute_percentage_error\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "import joblib\n",
    "\n",
    "# Mengabaikan peringatan saat load file audio (opsional tapi disarankan)\n",
    "warnings.filterwarnings('ignore', category=UserWarning, module='librosa')\n",
    "\n",
    "print(\"Cell 1: Semua library berhasil di-import.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b362acbb",
   "metadata": {},
   "source": [
    "## Standarisasi Audio (Convert ke Wav)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979a56c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!mkdir dataset_wav\n",
    "\n",
    "print(\"Memulai konversi audio ke .wav (16000 Hz, mono)...\")\n",
    "\n",
    "!for %f in (\"data\\buka fikri\\*.mp3\") do ffmpeg -i \"%f\" -ac 1 -ar 16000 \"dataset_wav\\%~nf.wav\"\n",
    "!for %f in (\"data\\tutup fikri\\*.mp3\") do ffmpeg -i \"%f\" -ac 1 -ar 16000 \"dataset_wav\\%~nf.wav\"\n",
    "!for %f in (\"data\\buka fauzan\\*.aac\") do ffmpeg -i \"%f\" -ac 1 -ar 16000 \"dataset_wav\\%~nf.wav\"\n",
    "!for %f in (\"data\\tutup fauzan\\*.aac\") do ffmpeg -i \"%f\" -ac 1 -ar 16000 \"dataset_wav\\%~nf.wav\"\n",
    "\n",
    "print(\"\\nStandardisasi selesai. Folder 'dataset_wav' siap.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77be0748",
   "metadata": {},
   "source": [
    "## Membuat Metadata Multilable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b3972e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "ORIGINAL_DATA_PATH = \"data/\" # Path ke folder 'data' Anda\n",
    "metadata_list = []\n",
    "\n",
    "# Definisikan pemetaan dari nama folder ke label multi-hot\n",
    "folder_mappings = {\n",
    "    # Pastikan nama string ini SAMA PERSIS dengan nama folder Anda\n",
    "    \"buka fikri\":  {'buka': 1, 'tutup': 0, 'fikri': 1, 'fauzan': 0},\n",
    "    \"tutup fikri\": {'buka': 0, 'tutup': 1, 'fikri': 1, 'fauzan': 0},\n",
    "    \"buka fauzan\": {'buka': 1, 'tutup': 0, 'fikri': 0, 'fauzan': 1},\n",
    "    \"tutup fauzan\":{'buka': 0, 'tutup': 1, 'fikri': 0, 'fauzan': 1}\n",
    "}\n",
    "\n",
    "print(f\"Membaca struktur folder asli dari {ORIGINAL_DATA_PATH}...\")\n",
    "\n",
    "# Loop melalui setiap folder yang kita definisikan\n",
    "for folder_name, labels in folder_mappings.items():\n",
    "    # Menggunakan os.path.join agar aman di Windows\n",
    "    folder_path = os.path.join(ORIGINAL_DATA_PATH, folder_name)\n",
    "    \n",
    "    if not os.path.isdir(folder_path):\n",
    "        print(f\"Warning: Folder '{folder_path}' tidak ditemukan. Dilewati.\")\n",
    "        continue\n",
    "        \n",
    "    print(f\"Memproses folder: {folder_name}\")\n",
    "    \n",
    "    # Loop melalui setiap file di dalam folder tersebut\n",
    "    for original_filename in os.listdir(folder_path):\n",
    "        # Dapatkan nama file tanpa ekstensi (.mp3 atau .aac)\n",
    "        base_name, ext = os.path.splitext(original_filename)\n",
    "        \n",
    "        # Kita hanya peduli file audio\n",
    "        if ext.lower() not in ['.mp3', '.aac']:\n",
    "            continue\n",
    "            \n",
    "        new_wav_filename = base_name + \".wav\"\n",
    "        \n",
    "        # Buat baris metadata baru\n",
    "        new_row = {\n",
    "            'file_name': new_wav_filename,\n",
    "            'buka': labels['buka'],\n",
    "            'tutup': labels['tutup'],\n",
    "            'fikri': labels['fikri'],\n",
    "            'fauzan': labels['fauzan']\n",
    "        }\n",
    "        metadata_list.append(new_row)\n",
    "\n",
    "# --- Simpan ke CSV ---\n",
    "if not metadata_list:\n",
    "    print(\"\\nERROR: Tidak ada file audio (.mp3/.aac) yang ditemukan di subfolder 'data/'.\")\n",
    "    print(\"Pastikan folder 'data' Anda ada di lokasi yang sama dengan notebook ini.\")\n",
    "else:\n",
    "    df_metadata = pd.DataFrame(metadata_list)\n",
    "    df_metadata.to_csv(\"metadata_anda.csv\", index=False)\n",
    "    \n",
    "    print(f\"\\nFile 'metadata_anda.csv' berhasil dibuat dengan {len(df_metadata)} baris.\")\n",
    "    print(\"\\nContoh 5 baris pertama:\")\n",
    "    display(df_metadata.head()) \n",
    "    print(\"\\nContoh 5 baris terakhir:\")\n",
    "    display(df_metadata.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77af83f3",
   "metadata": {},
   "source": [
    "## Memuat Data Audio ke Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c4d96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "METADATA_PATH = \"metadata_anda.csv\"\n",
    "AUDIO_FOLDER_PATH = \"dataset_wav/\"\n",
    "TARGET_SR = 16000\n",
    "\n",
    "print(f\"Membaca file metadata dari {METADATA_PATH}...\")\n",
    "try:\n",
    "    df_metadata = pd.read_csv(METADATA_PATH)\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: File {METADATA_PATH} tidak ditemukan.\")\n",
    "    raise\n",
    "\n",
    "data_list = []\n",
    "print(f\"Memulai proses pembacaan {len(df_metadata)} file audio dari {AUDIO_FOLDER_PATH}...\")\n",
    "\n",
    "for index, row in tqdm(df_metadata.iterrows(), total=df_metadata.shape[0]):\n",
    "    file_path = os.path.join(AUDIO_FOLDER_PATH, row['file_name'])\n",
    "    \n",
    "    try:\n",
    "        audio_signal, sampling_rate = librosa.load(file_path, sr=TARGET_SR)\n",
    "        \n",
    "        data_list.append({\n",
    "            'filename': row['file_name'],\n",
    "            'signal': audio_signal,\n",
    "            'sampling_rate': sampling_rate,\n",
    "            'label_buka': row['buka'],\n",
    "            'label_tutup': row['tutup'],\n",
    "            'label_fikri': row['fikri'],\n",
    "            'label_fauzan': row['fauzan']\n",
    "        })\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nWarning: Gagal membaca file {file_path}: {e}\")\n",
    "        \n",
    "print(f\"\\nProses pembacaan data selesai.\")\n",
    "print(f\"Total file berhasil dibaca: {len(data_list)}\")\n",
    "\n",
    "if data_list:\n",
    "    print(\"\\nContoh data pertama yang dimuat:\")\n",
    "    print(data_list[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97626fc0",
   "metadata": {},
   "source": [
    "## Ekstraksi Fitur TSFEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854c3d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def extract_features_tsfel(data_list):\n",
    "    cfg = tsfel.get_features_by_domain()\n",
    "    \n",
    "    feature_dfs = []\n",
    "    print(f\"\\nMemulai proses ekstraksi fitur TSFEL untuk {len(data_list)} file...\\n\")\n",
    "\n",
    "    for data in tqdm(data_list, total=len(data_list)):\n",
    "        signal = data['signal']\n",
    "        sr = data['sampling_rate']\n",
    "        \n",
    "        try:\n",
    "            features = tsfel.time_series_features_extractor(cfg, signal, fs=sr, verbose=0)\n",
    "            \n",
    "            features['filename'] = data['filename']\n",
    "            features['label_buka'] = data['label_buka']\n",
    "            features['label_tutup'] = data['label_tutup']\n",
    "            features['label_fikri'] = data['label_fikri']\n",
    "            features['label_fauzan'] = data['label_fauzan']\n",
    "            \n",
    "            feature_dfs.append(features)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\nError saat ekstraksi fitur {data['filename']}: {e}\")\n",
    "\n",
    "    if not feature_dfs:\n",
    "        print(\"Tidak ada fitur yang berhasil diekstrak.\")\n",
    "        return pd.DataFrame()\n",
    "        \n",
    "    feature_df = pd.concat(feature_dfs, ignore_index=True)\n",
    "    \n",
    "    print(f\"\\nEkstraksi fitur selesai!\")\n",
    "    print(f\"Total data fitur: {feature_df.shape[0]} baris, {feature_df.shape[1]} kolom\")\n",
    "    \n",
    "    return feature_df\n",
    "\n",
    "# --- Bagian Eksekusi ---\n",
    "if 'data_list' in locals():\n",
    "    print(\"Menjalankan ekstraksi fitur pada 'data_list' yang sudah dimuat...\")\n",
    "    features_df = extract_features_tsfel(data_list)\n",
    "    \n",
    "    print(\"\\nContoh 5 baris pertama dari DataFrame fitur (features_df):\")\n",
    "    display(features_df.head())\n",
    "else:\n",
    "    print(\"ERROR: Variabel 'data_list' tidak ditemukan. Jalankan ulang Cell 4.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e86e215",
   "metadata": {},
   "source": [
    "## Seleksi Fitur Information Gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15da56f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "def clean_and_select_features(features_df):\n",
    "    \"\"\"\n",
    "    Membersihkan data (NaN, inf) dan melakukan seleksi fitur\n",
    "    menggunakan Information Gain (Mutual Information).\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Memulai pembersihan data dan seleksi fitur...\")\n",
    "    \n",
    "    # 1. Pisahkan fitur (X) dan label (y)\n",
    "    label_cols = ['label_buka', 'label_tutup', 'label_fikri', 'label_fauzan']\n",
    "    # Kita hapus 'filename' dan semua kolom label\n",
    "    X = features_df.drop(columns=['filename'] + label_cols, errors='ignore')\n",
    "    y = features_df[label_cols]\n",
    "    \n",
    "    # Simpan nama fitur untuk nanti\n",
    "    feature_names = X.columns\n",
    "    \n",
    "    # 2. Tangani missing value (NaN) & infinity (inf)\n",
    "    # TSFEL kadang menghasilkan 'inf', ganti dengan 'NaN' dulu\n",
    "    X = X.replace([np.inf, -np.inf], np.nan)\n",
    "    \n",
    "    if X.isnull().sum().sum() > 0:\n",
    "        print(f\"Ditemukan {X.isnull().sum().sum()} missing values. Mengisi dengan median...\")\n",
    "        imputer = SimpleImputer(strategy='median')\n",
    "        # fit_transform mengembalikan numpy array, jadi kita simpan nama kolomnya\n",
    "        X_imputed = imputer.fit_transform(X)\n",
    "    else:\n",
    "        print(\"Tidak ada missing values. Melanjutkan...\")\n",
    "        X_imputed = X.values # Ubah ke numpy array agar konsisten\n",
    "\n",
    "    # 3. Hitung Information Gain (Mutual Information) untuk SETIAP label\n",
    "    # Kita TIDAK perlu LabelEncoder karena label kita sudah 0/1\n",
    "    print(\"Menghitung Information Gain untuk 4 label...\")\n",
    "    \n",
    "    # discrete_features='auto' bagus untuk menangani fitur TSFEL\n",
    "    ig_buka = mutual_info_classif(X_imputed, y['label_buka'], discrete_features='auto')\n",
    "    ig_tutup = mutual_info_classif(X_imputed, y['label_tutup'], discrete_features='auto')\n",
    "    ig_fikri = mutual_info_classif(X_imputed, y['label_fikri'], discrete_features='auto')\n",
    "    ig_fauzan = mutual_info_classif(X_imputed, y['label_fauzan'], discrete_features='auto')\n",
    "    \n",
    "    # 4. Gabungkan hasil ke DataFrame\n",
    "    ig_df = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'IG_Buka': ig_buka,\n",
    "        'IG_Tutup': ig_tutup,\n",
    "        'IG_Fikri': ig_fikri,\n",
    "        'IG_Fauzan': ig_fauzan\n",
    "    })\n",
    "    \n",
    "    # Buat kolom rata-rata IG untuk mengurutkan fitur terbaik secara keseluruhan\n",
    "    ig_df['IG_Average'] = ig_df[['IG_Buka', 'IG_Tutup', 'IG_Fikri', 'IG_Fauzan']].mean(axis=1)\n",
    "    \n",
    "    # Urutkan dari yang paling informatif\n",
    "    ig_df = ig_df.sort_values(by='IG_Average', ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    # --- 5. VISUALISASI (Sesuai permintaan Anda) ---\n",
    "    print(\"Membuat visualisasi untuk 20 fitur terbaik...\")\n",
    "    \n",
    "    top_n = 20\n",
    "    top_features = ig_df.head(top_n)\n",
    "    \n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.barplot(\n",
    "        x='IG_Average', \n",
    "        y='Feature', \n",
    "        data=top_features, \n",
    "        palette='viridis'\n",
    "    )\n",
    "    plt.title(f'Top {top_n} Fitur Paling Informatif (Rata-rata Information Gain)', fontsize=16)\n",
    "    plt.xlabel('Rata-rata Skor IG (Semakin tinggi semakin baik)', fontsize=12)\n",
    "    plt.ylabel('Nama Fitur (dari TSFEL)', fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.show() # Tampilkan plot!\n",
    "    \n",
    "    return ig_df, X_imputed, y\n",
    "\n",
    "# --- Bagian Eksekusi ---\n",
    "if 'features_df' in locals():\n",
    "    # Jalankan fungsi dan simpan hasilnya ke variabel baru\n",
    "    ig_df, X_imputed, y_labels = clean_and_select_features(features_df)\n",
    "    \n",
    "    print(\"\\nSeleksi fitur selesai.\")\n",
    "    print(\"Contoh 5 fitur TERBAIK berdasarkan rata-rata IG:\")\n",
    "    display(ig_df.head())\n",
    "    \n",
    "    # Simpan nama-nama fitur terbaik untuk digunakan di cell training\n",
    "    # Ambil 100 fitur terbaik (atau sesuai kebutuhan)\n",
    "    TOP_N_FEATURES = 100 \n",
    "    selected_feature_names = ig_df.head(TOP_N_FEATURES)['Feature'].tolist()\n",
    "    print(f\"\\nDisimpan {len(selected_feature_names)} nama fitur terbaik untuk training.\")\n",
    "    \n",
    "else:\n",
    "    print(\"ERROR: Variabel 'features_df' tidak ditemukan. Jalankan ulang Cell 5.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335d4b69",
   "metadata": {},
   "source": [
    "## Pelatihan & Menyimpan Model Multilabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec06c8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "print(\"Memulai persiapan data untuk pelatihan model...\")\n",
    "\n",
    "# 1. Ambil 100 fitur terbaik yang sudah kita pilih di cell sebelumnya\n",
    "# Pastikan variabel 'selected_feature_names' ada dari Cell 6\n",
    "if 'selected_feature_names' not in locals():\n",
    "    print(\"ERROR: 'selected_feature_names' tidak ditemukan. Jalankan ulang Cell 6.\")\n",
    "    raise NameError(\"'selected_feature_names' not defined\")\n",
    "\n",
    "if 'features_df' not in locals():\n",
    "    print(\"ERROR: 'features_df' tidak ditemukan. Jalankan ulang Cell 5.\")\n",
    "    raise NameError(\"'features_df' not defined\")\n",
    "\n",
    "# 2. Buat dataset final X dan y\n",
    "label_cols = ['label_buka', 'label_tutup', 'label_fikri', 'label_fauzan']\n",
    "X = features_df[selected_feature_names] # Hanya ambil 100 fitur terbaik\n",
    "y = features_df[label_cols]\n",
    "\n",
    "print(f\"Dataset final dibuat dengan {X.shape[1]} fitur dan {X.shape[0]} data.\")\n",
    "\n",
    "# 3. Lakukan pembersihan (Imputasi) HANYA pada 100 fitur ini\n",
    "# Ini penting agar data 'live' nanti bisa diproses dengan cara yang sama\n",
    "X = X.replace([np.inf, -np.inf], np.nan)\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "X_final = imputer.fit_transform(X)\n",
    "y_final = y.values # Konversi y ke numpy array\n",
    "\n",
    "# 4. Split data (Train/Test)\n",
    "# Kita akan gunakan 80% data untuk melatih, 20% untuk menguji\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_final, y_final, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Data dibagi: {len(X_train)} data latih, {len(X_test)} data uji.\")\n",
    "\n",
    "# 5. Definisikan dan Latih Model (Random Forest)\n",
    "# n_estimators=100 artinya kita membangun 100 \"pohon\"\n",
    "# random_state=42 agar hasilnya konsisten setiap kali dijalankan\n",
    "print(\"Memulai pelatihan RandomForestClassifier (Multilabel)...\")\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Melatih model!\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(\"Pelatihan model selesai!\")\n",
    "\n",
    "# 6. Evaluasi Model\n",
    "print(\"\\n--- HASIL EVALUASI MODEL PADA DATA UJI ---\")\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Akurasi (Subset Accuracy):\n",
    "# Ini adalah skor yang SANGAT KETAT. \n",
    "# Model harus menebak SEMUA 4 label dengan benar untuk dapat 1 poin.\n",
    "acc_subset = accuracy_score(y_test, y_pred)\n",
    "print(f\"Akurasi (Subset): {acc_subset*100:.2f}%\")\n",
    "print(\"(Skor ini mengukur seberapa sering model menebak SEMUA 4 label dengan sempurna)\")\n",
    "\n",
    "# Laporan Klasifikasi (Per Label):\n",
    "# Ini adalah metrik yang JAUH LEBIH BERGUNA.\n",
    "print(\"\\nLaporan Klasifikasi (Per Label):\")\n",
    "print(classification_report(y_test, y_pred, target_names=label_cols, zero_division=0))\n",
    "\n",
    "# 7. Simpan Model untuk Aplikasi Prediksi\n",
    "# Kita perlu menyimpan 3 hal:\n",
    "# a. Model yang sudah dilatih\n",
    "# b. Imputer (untuk membersihkan data baru)\n",
    "# c. Daftar 100 nama fitur (agar tahu fitur apa yang harus diekstrak)\n",
    "\n",
    "model_filename = 'model_suara.joblib'\n",
    "imputer_filename = 'imputer.joblib'\n",
    "features_filename = 'selected_features.joblib'\n",
    "\n",
    "joblib.dump(model, model_filename)\n",
    "joblib.dump(imputer, imputer_filename)\n",
    "joblib.dump(selected_feature_names, features_filename)\n",
    "\n",
    "print(f\"\\nModel berhasil disimpan ke: {model_filename}\")\n",
    "print(f\"Imputer berhasil disimpan ke: {imputer_filename}\")\n",
    "print(f\"Daftar fitur berhasil disimpan ke: {features_filename}\")\n",
    "print(\"\\nAnda sekarang siap membuat aplikasi prediksi!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc32d51",
   "metadata": {},
   "source": [
    "## Eksperimen PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ce2c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.impute import SimpleImputer\n",
    "import numpy as np\n",
    "\n",
    "print(\"Memulai eksperimen PCA...\")\n",
    "\n",
    "# 1. Pisahkan fitur dan target (menggunakan variabel dari cell kita sebelumnya)\n",
    "if 'features_df' not in locals():\n",
    "    print(\"ERROR: 'features_df' tidak ditemukan. Jalankan Cell 5.\")\n",
    "    raise NameError(\"'features_df' not defined\")\n",
    "\n",
    "if 'ig_df' not in locals():\n",
    "    print(\"ERROR: 'ig_df' tidak ditemukan. Jalankan Cell 6.\")\n",
    "    raise NameError(\"'ig_df' not defined\")\n",
    "    \n",
    "# y tidak benar-benar dipakai di cell ini, tapi kita definisikan saja\n",
    "y = features_df[['label_buka', 'label_tutup', 'label_fikri', 'label_fauzan']]\n",
    "\n",
    "# === SEMUA FITUR ===\n",
    "# Ambil semua fitur numerik dari features_df\n",
    "X_all = features_df.drop(columns=['filename', 'label_buka', 'label_tutup', 'label_fikri', 'label_fauzan'], errors='ignore')\n",
    "# Pastikan hanya kolom numerik\n",
    "X_all = X_all.select_dtypes(include=[np.number]).copy()\n",
    "print(f\"Jumlah data (baris): {len(X_all)}\")\n",
    "print(f\"Jumlah fitur total: {X_all.shape[1]}\")\n",
    "\n",
    "# === FITUR TERPILIH (TOP 10) ===\n",
    "# Ambil 10 fitur terbaik dari hasil Information Gain (Cell 6)\n",
    "# Kode teman Anda menggunakan 10 (iloc[:9] di screenshot Anda salah, harusnya iloc[:10])\n",
    "top_features = ig_df.head(10)['Feature'].tolist()\n",
    "X_selected = X_all[top_features]\n",
    "print(f\"Jumlah fitur terpilih (Top 10): {len(top_features)}\")\n",
    "\n",
    "\n",
    "# === 2. IMPUTASI NILAI HILANG (NaN) ===\n",
    "# Teman Anda pakai 'mean', kita ikuti (meskipun di Cell 6 kita pakai 'median')\n",
    "print(\"Melakukan imputasi (mean)...\")\n",
    "X_all = X_all.replace([np.inf, -np.inf], np.nan) # Penting!\n",
    "X_selected = X_selected.replace([np.inf, -np.inf], np.nan) # Penting!\n",
    "\n",
    "imputer_all = SimpleImputer(strategy='mean')\n",
    "imputer_selected = SimpleImputer(strategy='mean')\n",
    "\n",
    "X_all_imputed = pd.DataFrame(imputer_all.fit_transform(X_all), columns=X_all.columns)\n",
    "X_selected_imputed = pd.DataFrame(imputer_selected.fit_transform(X_selected), columns=X_selected.columns)\n",
    "\n",
    "# === 3. NORMALISASI (STANDARD SCALER) ===\n",
    "# PCA sangat sensitif terhadap skala, jadi ini langkah wajib untuk PCA\n",
    "print(\"Melakukan normalisasi (StandardScaler)...\")\n",
    "scaler_all = StandardScaler()\n",
    "scaler_selected = StandardScaler()\n",
    "\n",
    "X_all_scaled = scaler_all.fit_transform(X_all_imputed)\n",
    "X_selected_scaled = scaler_selected.fit_transform(X_selected_imputed)\n",
    "\n",
    "# === 4. PCA UNTUK SEMUA FITUR ===\n",
    "print(\"Melakukan PCA pada semua fitur...\")\n",
    "pca_all = PCA(n_components=None) # Ambil semua komponen\n",
    "X_all_pca = pca_all.fit_transform(X_all_scaled)\n",
    "\n",
    "explained_variance_all = pca_all.explained_variance_ratio_\n",
    "cum_var_all = np.cumsum(explained_variance_all)\n",
    "\n",
    "# === 5. PCA UNTUK FITUR TERPILIH (TOP 10) ===\n",
    "print(\"Melakukan PCA pada fitur terpilih...\")\n",
    "pca_selected = PCA(n_components=None) # Ambil semua komponen\n",
    "X_selected_pca = pca_selected.fit_transform(X_selected_scaled)\n",
    "\n",
    "explained_variance_selected = pca_selected.explained_variance_ratio_\n",
    "cum_var_selected = np.cumsum(explained_variance_selected)\n",
    "\n",
    "print(\"\\nEksperimen PCA Selesai.\")\n",
    "print(f\"Variabel 'cum_var_all' dan 'cum_var_selected' siap untuk visualisasi.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc97248",
   "metadata": {},
   "source": [
    "## Visualisasi PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa58a5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"Membuat visualisasi perbandingan PCA...\")\n",
    "\n",
    "# === 1. VISUALISASI PERBANDINGAN PCA ===\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Variabel 'cum_var_all' dan 'cum_var_selected' dibuat oleh Cell 8\n",
    "plt.plot(cum_var_all, label='Semua Fitur', marker='o')\n",
    "plt.plot(cum_var_selected, label='Fitur Terpilih (Top 10)', marker='s')\n",
    "\n",
    "plt.title('Perbandingan Cumulative Explained Variance PCA')\n",
    "plt.xlabel('Jumlah Komponen')\n",
    "plt.ylabel('Cumulative Explained Variance Ratio')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# === 2. RINGKASAN HASIL ===\n",
    "# Variabel-variabel ini juga dibuat oleh Cell 8\n",
    "print(\"\\nRingkasan PCA:\")\n",
    "print(f\"Total komponen (semua fitur): {X_all_pca.shape[1]}\")\n",
    "print(f\"Total komponen (fitur terpilih): {X_selected_pca.shape[1]}\")\n",
    "print(f\"Explained variance (semua fitur, 5 pertama): {explained_variance_all[:5]}\")\n",
    "print(f\"Explained variance (fitur terpilih, 5 pertama): {explained_variance_selected[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873551b2",
   "metadata": {},
   "source": [
    "## Visualisasi PCA 2D & 3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e64259",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from mpl_toolkits.mplot3d import Axes3D # Untuk plot 3D\n",
    "\n",
    "print(\"Memulai visualisasi PCA 2D dan 3D...\")\n",
    "\n",
    "# Pastikan 'X_all_imputed' dan 'y' dari Cell 6 ada\n",
    "if 'X_all_imputed' not in locals(): # Jika Anda mengikuti alur saya, ini akan jadi X_imputed dari Cell 6\n",
    "    print(\"WARNING: 'X_all_imputed' tidak ditemukan. Menggunakan 'X_imputed' dari Cell 6.\")\n",
    "    # Kita pakai X_imputed yang sudah bersih dari Cell 6\n",
    "    if 'X_imputed' not in locals():\n",
    "        print(\"ERROR: 'X_imputed' juga tidak ditemukan. Jalankan Cell 6 terlebih dahulu.\")\n",
    "        raise NameError(\"'X_imputed' not defined\")\n",
    "    X_for_pca = X_imputed\n",
    "else: # Jika Anda menggunakan X_all_imputed dari Cell 8\n",
    "    X_for_pca = X_all_imputed\n",
    "\n",
    "if 'y_labels' not in locals(): # Jika Anda mengikuti alur saya, ini akan jadi y_labels dari Cell 6\n",
    "    print(\"WARNING: 'y_labels' tidak ditemukan. Menggunakan 'y' dari Cell 6.\")\n",
    "    if 'y' not in locals():\n",
    "        print(\"ERROR: 'y' juga tidak ditemukan. Jalankan Cell 6 terlebih dahulu.\")\n",
    "        raise NameError(\"'y' not defined\")\n",
    "    y_for_pca = y\n",
    "else: # Jika Anda menggunakan y dari Cell 8\n",
    "    y_for_pca = y_labels\n",
    "\n",
    "# Pastikan data telah diskalakan untuk PCA\n",
    "# Kita akan gunakan scaler baru untuk konsistensi di sini\n",
    "scaler_pca_viz = StandardScaler()\n",
    "X_scaled_for_viz = scaler_pca_viz.fit_transform(X_for_pca)\n",
    "\n",
    "\n",
    "# === 1. PCA 2D ===\n",
    "pca_2d = PCA(n_components=2)\n",
    "components_2d = pca_2d.fit_transform(X_scaled_for_viz)\n",
    "\n",
    "df_2d = pd.DataFrame(data=components_2d, columns=['PC1', 'PC2'])\n",
    "df_2d['label_buka'] = y_for_pca['label_buka'].values\n",
    "df_2d['label_tutup'] = y_for_pca['label_tutup'].values\n",
    "df_2d['label_fikri'] = y_for_pca['label_fikri'].values\n",
    "df_2d['label_fauzan'] = y_for_pca['label_fauzan'].values\n",
    "\n",
    "# Visualisasi 2D berdasarkan Speaker (Fikri vs Fauzan)\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.scatterplot(\n",
    "    x=\"PC1\", y=\"PC2\",\n",
    "    hue=df_2d['label_fikri'].map({0: 'Fauzan', 1: 'Fikri'}), # Map 0/1 ke nama speaker\n",
    "    style=df_2d['label_fikri'].map({0: 'Fauzan', 1: 'Fikri'}),\n",
    "    palette='deep',\n",
    "    data=df_2d,\n",
    "    legend=\"full\",\n",
    "    alpha=0.8\n",
    ")\n",
    "plt.title('PCA 2D: Speaker (Fikri vs Fauzan)')\n",
    "plt.xlabel(f'Principal Component 1 ({pca_2d.explained_variance_ratio_[0]*100:.2f}%)')\n",
    "plt.ylabel(f'Principal Component 2 ({pca_2d.explained_variance_ratio_[1]*100:.2f}%)')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Visualisasi 2D berdasarkan Aksi (Buka vs Tutup)\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.scatterplot(\n",
    "    x=\"PC1\", y=\"PC2\",\n",
    "    hue=df_2d['label_buka'].map({0: 'Tutup', 1: 'Buka'}), # Map 0/1 ke nama aksi\n",
    "    style=df_2d['label_buka'].map({0: 'Tutup', 1: 'Buka'}),\n",
    "    palette='deep',\n",
    "    data=df_2d,\n",
    "    legend=\"full\",\n",
    "    alpha=0.8\n",
    ")\n",
    "plt.title('PCA 2D: Aksi (Buka vs Tutup)')\n",
    "plt.xlabel(f'Principal Component 1 ({pca_2d.explained_variance_ratio_[0]*100:.2f}%)')\n",
    "plt.ylabel(f'Principal Component 2 ({pca_2d.explained_variance_ratio_[1]*100:.2f}%)')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# === 2. PCA 3D ===\n",
    "pca_3d = PCA(n_components=3)\n",
    "components_3d = pca_3d.fit_transform(X_scaled_for_viz)\n",
    "\n",
    "df_3d = pd.DataFrame(data=components_3d, columns=['PC1', 'PC2', 'PC3'])\n",
    "df_3d['label_buka'] = y_for_pca['label_buka'].values\n",
    "df_3d['label_tutup'] = y_for_pca['label_tutup'].values\n",
    "df_3d['label_fikri'] = y_for_pca['label_fikri'].values\n",
    "df_3d['label_fauzan'] = y_for_pca['label_fauzan'].values\n",
    "\n",
    "# Visualisasi 3D berdasarkan Speaker (Fikri vs Fauzan)\n",
    "fig_3d_speaker = plt.figure(figsize=(12, 10))\n",
    "ax_3d_speaker = fig_3d_speaker.add_subplot(111, projection='3d')\n",
    "\n",
    "# Warna dan marker berdasarkan speaker\n",
    "colors_speaker = df_3d['label_fikri'].map({0: 'red', 1: 'blue'})\n",
    "markers_speaker = df_3d['label_fikri'].map({0: 'o', 1: '^'})\n",
    "\n",
    "for i, txt in enumerate(df_3d.index):\n",
    "    ax_3d_speaker.scatter(\n",
    "        df_3d['PC1'][i], df_3d['PC2'][i], df_3d['PC3'][i],\n",
    "        color=colors_speaker[i],\n",
    "        marker=markers_speaker[i],\n",
    "        s=50, alpha=0.7\n",
    "    )\n",
    "\n",
    "ax_3d_speaker.set_xlabel(f'Principal Component 1 ({pca_3d.explained_variance_ratio_[0]*100:.2f}%)')\n",
    "ax_3d_speaker.set_ylabel(f'Principal Component 2 ({pca_3d.explained_variance_ratio_[1]*100:.2f}%)')\n",
    "ax_3d_speaker.set_zlabel(f'Principal Component 3 ({pca_3d.explained_variance_ratio_[2]*100:.2f}%)')\n",
    "ax_3d_speaker.set_title('PCA 3D: Speaker (Fikri = biru segitiga, Fauzan = merah bulat)')\n",
    "plt.show()\n",
    "\n",
    "# Visualisasi 3D berdasarkan Aksi (Buka vs Tutup)\n",
    "fig_3d_aksi = plt.figure(figsize=(12, 10))\n",
    "ax_3d_aksi = fig_3d_aksi.add_subplot(111, projection='3d')\n",
    "\n",
    "# Warna dan marker berdasarkan aksi\n",
    "colors_aksi = df_3d['label_buka'].map({0: 'green', 1: 'purple'})\n",
    "markers_aksi = df_3d['label_buka'].map({0: 's', 1: 'D'}) # s=persegi, D=diamond\n",
    "\n",
    "for i, txt in enumerate(df_3d.index):\n",
    "    ax_3d_aksi.scatter(\n",
    "        df_3d['PC1'][i], df_3d['PC2'][i], df_3d['PC3'][i],\n",
    "        color=colors_aksi[i],\n",
    "        marker=markers_aksi[i],\n",
    "        s=50, alpha=0.7\n",
    "    )\n",
    "\n",
    "ax_3d_aksi.set_xlabel(f'Principal Component 1 ({pca_3d.explained_variance_ratio_[0]*100:.2f}%)')\n",
    "ax_3d_aksi.set_ylabel(f'Principal Component 2 ({pca_3d.explained_variance_ratio_[1]*100:.2f}%)')\n",
    "ax_3d_aksi.set_zlabel(f'Principal Component 3 ({pca_3d.explained_variance_ratio_[2]*100:.2f}%)')\n",
    "ax_3d_aksi.set_title('PCA 3D: Aksi (Buka = ungu diamond, Tutup = hijau persegi)')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nVisualisasi PCA 2D dan 3D selesai.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9878a71",
   "metadata": {},
   "source": [
    "## Persiapan Dataset Untuk Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0eeee85",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "print(\"Mempersiapkan 3 dataset final untuk evaluasi...\")\n",
    "\n",
    "# Ambil variabel-variabel yang sudah kita buat di cell-cell sebelumnya\n",
    "# Pastikan 'X_selected_imputed', 'X_all_pca', 'X_selected_pca', dan 'y_labels' ada.\n",
    "if 'y_labels' not in locals():\n",
    "    print(\"ERROR: 'y_labels' tidak ditemukan. Jalankan ulang Cell 6.\")\n",
    "    raise NameError(\"'y_labels' not defined\")\n",
    "\n",
    "# === 1. Dataset hasil Information Gain (fitur terpilih Top 10) ===\n",
    "# 'X_selected_imputed' dibuat di Cell 8\n",
    "# 'y_labels' (DataFrame) dibuat di Cell 6\n",
    "df_ig_ready = pd.concat([\n",
    "    pd.DataFrame(X_selected_imputed, columns=top_features), \n",
    "    y_labels.reset_index(drop=True) # Gabungkan 4 kolom label\n",
    "], axis=1)\n",
    "\n",
    "\n",
    "# === 2. Dataset hasil PCA dari semua fitur ===\n",
    "# 'X_all_pca' dibuat di Cell 8\n",
    "# Beri nama kolom PC1, PC2, ...\n",
    "pca_all_cols = [f\"PC{i+1}\" for i in range(X_all_pca.shape[1])]\n",
    "df_pca_all_ready = pd.concat([\n",
    "    pd.DataFrame(X_all_pca, columns=pca_all_cols),\n",
    "    y_labels.reset_index(drop=True) # Gabungkan 4 kolom label\n",
    "], axis=1)\n",
    "\n",
    "\n",
    "# === 3. Dataset hasil PCA dari fitur terpilih (Top 10) ===\n",
    "# 'X_selected_pca' dibuat di Cell 8\n",
    "pca_selected_cols = [f\"PC{i+1}\" for i in range(X_selected_pca.shape[1])]\n",
    "df_pca_selected_ready = pd.concat([\n",
    "    pd.DataFrame(X_selected_pca, columns=pca_selected_cols),\n",
    "    y_labels.reset_index(drop=True) # Gabungkan 4 kolom label\n",
    "], axis=1)\n",
    "\n",
    "\n",
    "# === 4. Cetak ringkasan untuk verifikasi ===\n",
    "print(\"\\nDataset siap pakai:\")\n",
    "print(f\"- df_ig_ready: {df_ig_ready.shape} (fitur terpilih IG)\")\n",
    "print(f\"- df_pca_all_ready: {df_pca_all_ready.shape} (PCA semua fitur)\")\n",
    "print(f\"- df_pca_selected_ready: {df_pca_selected_ready.shape} (PCA fitur terpilih)\")\n",
    "\n",
    "# Verifikasi salah satu dataset\n",
    "print(\"\\nContoh 5 baris terakhir dari 'df_pca_selected_ready' (termasuk label):\")\n",
    "display(df_pca_selected_ready.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea36d76",
   "metadata": {},
   "source": [
    "## Modeling dan Hasil Evaluasi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e886e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold, cross_validate\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# === 1. Membuat Kolom Baru untuk Target (Single-Label) ===\n",
    "print(\"Membuat kolom 'target' gabungan (single-label)...\")\n",
    "\n",
    "# Kita butuh fungsi untuk mengubah 4 label [1,0,1,0] menjadi string \"buka_fikri\"\n",
    "def create_target_string(row):\n",
    "    aksi = \"buka\" if row['label_buka'] == 1 else \"tutup\"\n",
    "    identitas = \"fikri\" if row['label_fikri'] == 1 else \"fauzan\"\n",
    "    return f\"{aksi}_{identitas}\"\n",
    "\n",
    "# Ambil 'df_ig_ready' (atau DataFrame lain, isinya sama 400 baris)\n",
    "# Terapkan fungsi untuk membuat 1 seri target gabungan\n",
    "target_series = df_ig_ready.apply(create_target_string, axis=1)\n",
    "\n",
    "# Tambahkan seri 'target' ini ke 3 DataFrame yang sudah kita siapkan\n",
    "df_ig_ready['target'] = target_series\n",
    "df_pca_all_ready['target'] = target_series\n",
    "df_pca_selected_ready['target'] = target_series\n",
    "\n",
    "print(\"Kolom 'target' baru (misal: 'buka_fikri') telah ditambahkan ke 3 DataFrame.\")\n",
    "print(f\"Contoh target: {df_ig_ready['target'].unique()}\")\n",
    "\n",
    "\n",
    "# === 2. Membandingkan Models KNN, RF, SVM ===\n",
    "print(\"\\nMemulai perbandingan model...\")\n",
    "\n",
    "# 1. Pilih dataset yang ingin digunakan (sesuai screenshot teman Anda)\n",
    "dataset = df_pca_selected_ready\n",
    "# Anda bisa ganti di atas menjadi:\n",
    "# dataset = df_ig_ready\n",
    "# dataset = df_pca_all_ready\n",
    "\n",
    "print(f\"Menggunakan dataset: 'df_pca_selected_ready' (shape: {dataset.shape})\")\n",
    "\n",
    "# 2. Pisahkan fitur dan target\n",
    "# Kita buang semua kolom label, sisakan fitur (PC1, PC2, ...)\n",
    "label_cols_to_drop = ['label_buka', 'label_tutup', 'label_fikri', 'label_fauzan', 'target']\n",
    "X = dataset.drop(columns=label_cols_to_drop, errors='ignore')\n",
    "y_string = dataset[\"target\"]\n",
    "\n",
    "# 3. Encode label (misal: 'buka_fikri' -> 0, 'tutup_fikri' -> 1, dst.)\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y_string)\n",
    "print(f\"Label telah di-encode. {len(le.classes_)} kelas: {le.classes_}\")\n",
    "\n",
    "# 4. Inisialisasi model-model yang akan dibandingkan\n",
    "models = {\n",
    "    \"KNN\": KNeighborsClassifier(n_neighbors=5),\n",
    "    \"SVM (RBF)\": SVC(kernel=\"rbf\", C=1, gamma=\"scale\", random_state=42),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "}\n",
    "\n",
    "# 5. Konfigurasi K-fold Cross Validation\n",
    "# (n_splits=5 berarti 5-fold CV)\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# 6. Evaluasi setiap model\n",
    "results = []\n",
    "scoring_metrics = ['accuracy', 'precision_macro', 'recall_macro', 'f1_macro']\n",
    "print(\"Menjalankan 5-fold Cross Validation untuk setiap model...\")\n",
    "\n",
    "for name, model in models.items():\n",
    "    scores = cross_validate(\n",
    "        model, X, y, cv=kfold,\n",
    "        scoring=scoring_metrics\n",
    "    )\n",
    "    \n",
    "    results.append({\n",
    "        \"Model\": name,\n",
    "        \"Akurasi (mean)\": np.mean(scores['test_accuracy']),\n",
    "        \"Precision (mean)\": np.mean(scores['test_precision_macro']),\n",
    "        \"Recall (mean)\": np.mean(scores['test_recall_macro']),\n",
    "        \"F1-Score (mean)\": np.mean(scores['test_f1_macro']),\n",
    "        \"std Akurasi\": np.std(scores['test_accuracy'])\n",
    "    })\n",
    "    \n",
    "    print(f\"  {name:15s} | Acc: {np.mean(scores['test_accuracy']):.4f} | F1: {np.mean(scores['test_f1_macro']):.4f}\")\n",
    "\n",
    "# 7. Simpan hasil ke DataFrame dan tampilkan\n",
    "print(\"\\n--- Hasil Perbandingan Model ---\")\n",
    "results_df = pd.DataFrame(results).sort_values(by=\"Akurasi (mean)\", ascending=False)\n",
    "display(results_df)\n",
    "\n",
    "# 8. Visualisasi hasil perbandingan\n",
    "print(\"\\nMembuat visualisasi perbandingan akurasi...\")\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.bar(\n",
    "    results_df[\"Model\"], \n",
    "    results_df[\"Akurasi (mean)\"], \n",
    "    yerr=results_df[\"std Akurasi\"], # Tambahkan error bar\n",
    "    capsize=5, \n",
    "    color=['blue', 'green', 'red']\n",
    ")\n",
    "plt.title(\"Perbandingan Akurasi Model (5-Fold Cross-Validation)\")\n",
    "plt.ylabel(\"Akurasi (mean)\")\n",
    "plt.xlabel(\"Model\")\n",
    "plt.ylim(0, 1) # Skala akurasi dari 0 sampai 1\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3088ef",
   "metadata": {},
   "source": [
    "## Visualisasi Hasil Akhir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ddb7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CELL 12: VISUALISASI AKHIR & PENYIMPANAN OBJEK (Pengganti Cell 10 Teman Anda) ---\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "\n",
    "# Pastikan 'results_df' ada dari Cell 11\n",
    "if 'results_df' not in locals():\n",
    "    print(\"ERROR: 'results_df' tidak ditemukan. Jalankan ulang Cell 11.\")\n",
    "    raise NameError(\"'results_df' not defined\")\n",
    "\n",
    "# === 1. VISUALISASI PERBANDINGAN MULTI-METRIK ===\n",
    "print(\"Membuat visualisasi perbandingan multi-metrik...\")\n",
    "\n",
    "# Daftar metrik yang ingin kita plot (sesuai dengan kolom di results_df)\n",
    "metrics = [\"Akurasi (mean)\", \"Precision (mean)\", \"Recall (mean)\", \"F1-Score (mean)\"]\n",
    "\n",
    "# Kita \"unpivot\" DataFrame agar mudah di-plot\n",
    "results_df_melted = results_df.melt(\n",
    "    id_vars=[\"Model\"], \n",
    "    value_vars=metrics, \n",
    "    var_name=\"Metrik\", \n",
    "    value_name=\"Nilai\"\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(9, 5))\n",
    "# Loop untuk setiap model dan buat plot garisnya\n",
    "for model_name in results_df[\"Model\"].unique():\n",
    "    # Ambil data hanya untuk model ini\n",
    "    model_data = results_df_melted[results_df_melted[\"Model\"] == model_name]\n",
    "    \n",
    "    plt.plot(\n",
    "        model_data[\"Metrik\"], \n",
    "        model_data[\"Nilai\"], \n",
    "        marker='o', \n",
    "        label=model_name\n",
    "    )\n",
    "\n",
    "plt.title(\"Perbandingan Multi-Metrik Tiap Model (pada Target Single-Label)\")\n",
    "plt.ylabel(\"Nilai Skor\")\n",
    "plt.ylim(0, 1) # Skala dari 0 sampai 1\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.show()\n",
    "\n",
    "# === 2. MENYIMPAN OBJEK DARI ALUR PCA (Eksperimen Dosen) ===\n",
    "print(\"\\nMenyimpan objek dari alur eksperimen PCA...\")\n",
    "\n",
    "# Pastikan objek-objek dari Cell 8 ada\n",
    "try:\n",
    "    # Buat folder untuk menyimpan model eksperimen\n",
    "    !mkdir saved_models_eksperimen\n",
    "    \n",
    "    # Menyimpan objek PCA dari fitur terpilih\n",
    "    joblib.dump(pca_selected, \"saved_models_eksperimen/pca_selected.joblib\")\n",
    "    \n",
    "    # Menyimpan objek Scaler dari fitur terpilih\n",
    "    joblib.dump(scaler_selected, \"saved_models_eksperimen/scaler_selected.joblib\")\n",
    "    \n",
    "    # Menyimpan salah satu model (misal RF) dari Cell 11\n",
    "    joblib.dump(models[\"Random Forest\"], \"saved_models_eksperimen/model_rf_single_label_pca.joblib\")\n",
    "    \n",
    "    print(\"Objek PCA, Scaler, dan Model (RF) dari alur eksperimen berhasil disimpan.\")\n",
    "\n",
    "except NameError as e:\n",
    "    print(f\"ERROR: Gagal menyimpan objek. Variabel tidak ditemukan: {e}\")\n",
    "    print(\"Pastikan Cell 8 dan Cell 11 sudah dijalankan.\")\n",
    "except Exception as e:\n",
    "    print(f\"Terjadi error saat menyimpan: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
